name: Son Eklenen Dizileri Kontrol Etme

on:
  schedule:
    - cron: "0 */6 * * *"  # Her 6 saatte bir
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Python'u Kur
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Gerekli KÃ¼tÃ¼phaneleri YÃ¼kle
        run: |
          pip install requests beautifulsoup4

      - name: Scraper'Ä± Ã‡alÄ±ÅŸtÄ±r
        run: |
          python - <<EOF
          import requests
          from bs4 import BeautifulSoup

          def safe_get(url, headers):
              try:
                  return requests.get(url, headers=headers, timeout=10)
              except requests.exceptions.RequestException:
                  return None

          # ----------------------------------------------------
          # 1) DIZIPALL
          print("\n=== DIZIPALL ===")
          domain_url = "https://raw.githubusercontent.com/zerodayip/domain/refs/heads/main/dizipall.txt"
          domain = requests.get(domain_url).text.strip()

          url = f"{domain}/kesfet"
          headers = {
              "User-Agent": "Mozilla/5.0",
              "Referer": domain
          }

          response = safe_get(url, headers)

          if response and response.status_code == 200:
              soup = BeautifulSoup(response.text, "html.parser")
              print(f"ðŸŒ {domain}")

              items = soup.find_all("div", class_="filter-result-box")

              for item in items:
                  title = item.find("h2").get_text(strip=True).upper() if item.find("h2") else "BÄ°LÄ°NMÄ°YOR"

                  year = "BÄ°LÄ°NMÄ°YOR"
                  bottom = item.find("div", class_="filter-result-box-bottom")
                  if bottom:
                      p = bottom.find("p")
                      if p:
                          year = p.get_text(strip=True)

                  print(f"{title} - {year}")

              print("---------------------")
          else:
              print(f"âŒ {domain} ulaÅŸÄ±lamÄ±yor (site kapanmÄ±ÅŸ olabilir).")

          # ----------------------------------------------------
          # 2) DIZIYIIZLE
          print("\n=== DIZIYIIZLE ===")
          domain_url2 = "https://raw.githubusercontent.com/zerodayip/domain/refs/heads/main/diziyiizle.txt"
          domain2 = requests.get(domain_url2).text.strip()

          url2 = f"{domain2}/kesfet-izle/"
          headers2 = {
              "User-Agent": "Mozilla/5.0",
              "Referer": domain2
          }

          response2 = safe_get(url2, headers2)

          if response2 and response2.status_code == 200:
              soup2 = BeautifulSoup(response2.text, "html.parser")
              print(f"ðŸŒ {domain2}")

              cards = soup2.select("#seriesGrid a")

              for card in cards:
                  img = card.find("img")
                  title = img.get("alt", "BÄ°LÄ°NMÄ°YOR").upper() if img else "BÄ°LÄ°NMÄ°YOR"

                  year = "BÄ°LÄ°NMÄ°YOR"
                  year_tag = card.find("div", class_="flex items-center space-x-2 text-sm text-gray-400")
                  if year_tag:
                      span = year_tag.find("span")
                      if span:
                          year = span.get_text(strip=True)

                  print(f"{title} - {year}")

              print("---------------------")
          else:
              print(f"âŒ {domain2} ulaÅŸÄ±lamÄ±yor (site kapanmÄ±ÅŸ olabilir).")

          # ----------------------------------------------------
          # 3) DIZIPAL2 (domain3)
          print("\n=== DIZIPAL2 ===")
          domain_url3 = "https://raw.githubusercontent.com/zerodayip/domain/refs/heads/main/dizipal2.txt"
          domain3 = requests.get(domain_url3).text.strip()
          print(f"ðŸŒ {domain3}")

          url3 = f"{domain3}/"
          headers3 = {
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
              "Accept-Language": "tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7",
              "Referer": "https://google.com/",
              "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
              "Connection": "keep-alive"
          }

          response3 = safe_get(url3, headers3)

          if response3 and response3.status_code == 200:
              soup3 = BeautifulSoup(response3.text, "html.parser")

              target_div = soup3.find(
                  "div",
                  {
                      "class": "aa-cn",
                      "id": "widget_list_movies_series-2-aa-movies"
                  }
              )

              if target_div:
                  titles = target_div.find_all("h2", class_="entry-title")
                  for title in titles:
                      print(title.get_text(strip=True))
              else:
                  print("Div bulunamadÄ±.")
          else:
              print(f"âŒ {domain3} ulaÅŸÄ±lamÄ±yor (site kapanmÄ±ÅŸ olabilir).")

          EOF
