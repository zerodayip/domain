name: Dizi Scraper

on:
  schedule:
    - cron: "0 */6 * * *"  # Her 6 saatte bir
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Python'u Kur
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Gerekli KÃ¼tÃ¼phaneleri YÃ¼kle
        run: |
          pip install requests beautifulsoup4 cloudscraper

      - name: Scraper'Ä± Ã‡alÄ±ÅŸtÄ±r
        run: |
          python - <<EOF
          import requests
          import cloudscraper
          from bs4 import BeautifulSoup

          # -------------------------------
          # 1. DIZI20.LIFE
          domain_url1 = "https://raw.githubusercontent.com/zerodayip/domain/refs/heads/main/dizilife.txt"
          domain1 = requests.get(domain_url1).text.strip()
          url1 = f"{domain1}/diziler?sort=newest"
          headers = {
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                            "AppleWebKit/537.36 (KHTML, like Gecko) "
                            "Chrome/128.0.0.0 Safari/537.36",
              "Referer": domain1
          }
          response1 = requests.get(url1, headers=headers)
          if response1.status_code == 200:
              soup1 = BeautifulSoup(response1.text, "html.parser")
              cards = soup1.find_all("div", class_="content-card")
              print(f"ðŸŒ {domain1}")
              for card in cards:
                  img = card.find("img")
                  meta = card.find("div", class_="card-meta")
                  if img and meta:
                      title = img.get("alt", "BÄ°LÄ°NMÄ°YOR").strip().upper()
                      year_span = meta.find("span")
                      year = year_span.get_text(strip=True) if year_span else "BÄ°LÄ°NMÄ°YOR"
                      print(f"{title} - {year}")
              print("---------------------")
          else:
              print(f"{domain1} isteÄŸi baÅŸarÄ±sÄ±z:", response1.status_code)

          # -------------------------------
          # 2. FILMHANE.ONLINE (cloudscraper ile)
          domain_url2 = "https://raw.githubusercontent.com/zerodayip/domain/refs/heads/main/filmhane.txt"
          domain2 = requests.get(domain_url2).text.strip()
          url2 = f"{domain2}/kesfet/eyJ0eXBlIjoic2VyaWVzIn0="

          # Cloudflare bypass
          scraper = cloudscraper.create_scraper(
              browser={"browser": "chrome", "platform": "windows", "mobile": False}
          )
          scraper.headers.update({
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                            "AppleWebKit/537.36 (KHTML, like Gecko) "
                            "Chrome/129.0.6668.100 Safari/537.36",
              "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
              "Accept-Language": "tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7",
              "Referer": "https://google.com/",
              "Connection": "keep-alive",
              "Upgrade-Insecure-Requests": "1"
          })

          response2 = scraper.get(url2)
          if response2.status_code == 200:
              soup2 = BeautifulSoup(response2.text, "html.parser")
              results = soup2.find_all("div", class_="filter-result-box")
              print(f"ðŸŒ {domain2}")
              for item in results:
                  title_tag = item.find("h2")
                  year = "BÄ°LÄ°NMÄ°YOR"
                  bottom = item.find("div", class_="filter-result-box-bottom")
                  if bottom:
                      p = bottom.find("p")
                      if p:
                          year = p.get_text(strip=True)
                  if title_tag:
                      title = title_tag.get_text(strip=True).upper()
                      print(f"{title} - {year}")
              print("---------------------")
          else:
              print(f"{domain2} isteÄŸi baÅŸarÄ±sÄ±z:", response2.status_code)

          # -------------------------------
          # 3. DIZIYIIZLE
          domain_url3 = "https://raw.githubusercontent.com/zerodayip/domain/refs/heads/main/diziyiizle.txt"
          domain3 = requests.get(domain_url3).text.strip()
          url3 = f"{domain3}/kesfet-izle/"
          headers = {
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                            "AppleWebKit/537.36 (KHTML, like Gecko) "
                            "Chrome/128.0.0.0 Safari/537.36",
              "Referer": domain3
          }
          response3 = requests.get(url3, headers=headers)
          if response3.status_code == 200:
              soup3 = BeautifulSoup(response3.text, "html.parser")
              print(f"ðŸŒ {domain3}")
              cards = soup3.select("#seriesGrid a")
              for card in cards:
                  title_tag = card.find("img")
                  title = title_tag.get("alt", "BÄ°LÄ°NMÄ°YOR").strip().upper() if title_tag else "BÄ°LÄ°NMÄ°YOR"
                  year_tag = card.find("div", class_="flex items-center space-x-2 text-sm text-gray-400")
                  year = "BÄ°LÄ°NMÄ°YOR"
                  if year_tag:
                      span = year_tag.find("span")
                      if span:
                          year = span.get_text(strip=True)
                  print(f"{title} - {year}")
              print("---------------------")
          else:
              print(f"{domain3} isteÄŸi baÅŸarÄ±sÄ±z:", response3.status_code)
          EOF
